"""
Author: é‚¹è‡´è¿œ
Email: www.pisyongheng@foxmail.com
Date Created: 2025/8/30
Last Updated: 2025/8/30
Version: 1.0.1
"""
import pandas as pd
import tiktoken  # å¯¼å…¥ tiktoken ç”¨äºè®¡ç®— token æ•°é‡
from openai import OpenAI
from langchain.docstore.document import Document
from langchain_community.vectorstores import Chroma
from langchain.embeddings.base import Embeddings
from tqdm import tqdm


# ============ è‡ªå®šä¹‰ DashScope Embeddings ç±» ============

class DashScopeEmbeddings(Embeddings):
    def __init__(self,
                 api_key: str = "sk-xxxx",  # æ›¿æ¢ä¸ºä½ çš„API Key
                 base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
                 model: str = "text-embedding-v4",
                 dimensions: int = 1024,
                 max_token_limit: int = 8192):  # æœ€å¤§ token é™åˆ¶ï¼Œé»˜è®¤ 8192
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.dimensions = dimensions
        self.max_token_limit = max_token_limit
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

        # ä½¿ç”¨ tiktoken åŠ è½½ tokenizer
        self.encoder = tiktoken.get_encoding("cl100k_base")  # é€‚é… GPT-3.5 å’Œ GPT-4

    def embed_documents(self, texts):
        embeddings = []
        skipped_texts = []  # è®°å½•è·³è¿‡çš„æ–‡æœ¬

        for text in tqdm(texts, desc="åµŒå…¥æ–‡æ¡£", unit="æ–‡æ¡£"):
            # åˆ¤æ–­æ–‡æœ¬æ˜¯å¦è¶…é•¿ï¼Œè‹¥è¶…é•¿åˆ™è·³è¿‡
            if self._is_text_too_long(text):
                skipped_texts.append(text[:100])  # è®°å½•è·³è¿‡çš„æ–‡æœ¬å‰100ä¸ªå­—ç¬¦
                print(f"âš ï¸ è·³è¿‡ï¼šæ–‡æœ¬è¶…é•¿ï¼Œæ— æ³•å¤„ç†è¯¥æ–‡æœ¬ï¼š{text[:100]}...")
                continue  # è·³è¿‡è¶…é•¿æ–‡æœ¬

            # å¤„ç†æ­£å¸¸çš„æ–‡æœ¬
            embedding = self._embed(text)
            embeddings.append(embedding)

        # æ‰“å°è·³è¿‡çš„æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
        if skipped_texts:
            print(f"å…±è·³è¿‡ {len(skipped_texts)} ä¸ªè¶…é•¿æ–‡æœ¬ï¼š")
            for skipped in skipped_texts:
                print(f"  - {skipped}")

        return embeddings

    def embed_query(self, text):
        if self._is_text_too_long(text):
            print(f"âš ï¸ è·³è¿‡ï¼šæŸ¥è¯¢æ–‡æœ¬è¶…é•¿ï¼Œæ— æ³•å¤„ç†è¯¥æŸ¥è¯¢ï¼š{text[:100]}...")
            return None  # è¿”å› None è¡¨ç¤ºæ— æ³•å¤„ç†è¯¥æŸ¥è¯¢

        # æ­£å¸¸å¤„ç†æŸ¥è¯¢æ–‡æœ¬
        return self._embed(text)

    def _embed(self, text: str):
        # ä½¿ç”¨ OpenAI API ç”ŸæˆåµŒå…¥
        resp = self.client.embeddings.create(
            model=self.model,
            input=text,
            dimensions=self.dimensions,
            encoding_format="float"
        )
        return resp.data[0].embedding

    def _is_text_too_long(self, text: str):
        # è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡
        token_count = len(self.encoder.encode(text))
        return token_count > self.max_token_limit

# ============ æ„å»ºçŸ¥è¯†åº“ ============

def build_knowledge_base(excel_file: str, save_path: str, batch_size: int = 100):
    """
    æ„å»ºæˆ–æ›´æ–°çŸ¥è¯†åº“ï¼Œæ”¯æŒä»ä¸­æ–­å¤„ç»§ç»­ï¼Œä¸ä¼šé‡å¤æ·»åŠ ã€‚
    """
    # 1. åˆå§‹åŒ– Embedding æ¨¡å‹
    embeddings = DashScopeEmbeddings()

    # 2. åŠ è½½æˆ–åˆ›å»ºæ•°æ®åº“
    db = Chroma(persist_directory=save_path, embedding_function=embeddings,
                collection_metadata={"hnsw:space": "cosine"})

    # 3. è·å–æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„æ‰€æœ‰æ–‡æ¡£IDï¼Œè¿™æ˜¯å®ç°æ–­ç‚¹ç»­ä¼ çš„å…³é”®ï¼
    # ä½¿ç”¨ set æ˜¯ä¸ºäº†è®©åç»­çš„æŸ¥è¯¢é€Ÿåº¦æ›´å¿« (O(1))
    existing_ids = set(db.get()['ids'])
    print(f"âœ… çŸ¥è¯†åº“åŠ è½½æˆåŠŸã€‚å‘ç°å·²å­˜åœ¨ {len(existing_ids)} ä¸ªæ–‡æ¡£ã€‚")

    # 4. è¯»å–Excelå¹¶ç­›é€‰å‡ºéœ€è¦æ–°å¢çš„æ–‡æ¡£
    df = pd.read_excel(excel_file)
    if "req_code" not in df.columns:
        raise ValueError("Excelä¸­å¿…é¡»åŒ…å« req_code åˆ—ï¼")

    docs_to_add = []
    ids_to_add = []  # æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå•ç‹¬çš„åˆ—è¡¨æ¥å­˜æ”¾ä¸ docs_to_add å¯¹åº”çš„ID

    print("ğŸ” å¼€å§‹æ£€æŸ¥Excelæ–‡ä»¶ï¼Œç­›é€‰éœ€è¦æ–°å¢çš„æ–‡æ¡£...")
    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc="ç­›é€‰æ–°æ–‡æ¡£", unit="è¡Œ"):
        req_code = str(row["req_code"]).strip()

        # å¦‚æœ req_code å·²ç»å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œåˆ™è·³è¿‡
        if req_code in existing_ids:
            continue

        # å¦‚æœæ˜¯æ–°æ–‡æ¡£ï¼Œåˆ™å¤„ç†å¹¶å‡†å¤‡æ·»åŠ 
        content_parts = [f"{col}: {row[col]}" for col in df.columns if col != "req_code"]
        content = "\n".join(content_parts)

        # åŒæ ·éœ€è¦æ£€æŸ¥æ–‡æ¡£æ˜¯å¦è¶…é•¿
        if embeddings._is_text_too_long(content):
            print(f"âš ï¸ è·³è¿‡è¶…é•¿æ–‡æ¡£: {content[:100]}...")
            continue

        docs_to_add.append(Document(page_content=req_code, metadata={"content": content}))
        ids_to_add.append(req_code)

    # 5. å¦‚æœæ²¡æœ‰æ–°æ–‡æ¡£éœ€è¦æ·»åŠ ï¼Œåˆ™ç›´æ¥é€€å‡º
    if not docs_to_add:
        print("ğŸ‰ æ‰€æœ‰æ–‡æ¡£å‡å·²å­˜åœ¨äºçŸ¥è¯†åº“ä¸­ï¼Œæ— éœ€æ–°å¢ã€‚")
        return

    print(f"â„¹ï¸ å‘ç° {len(docs_to_add)} ä¸ªæ–°æ–‡æ¡£éœ€è¦è¢«æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚")

    # 6. åˆ†æ‰¹æ¬¡å°†æ–°æ–‡æ¡£åŠå…¶å”¯ä¸€IDæ·»åŠ åˆ°æ•°æ®åº“
    total_docs = len(docs_to_add)
    for i in tqdm(range(0, total_docs, batch_size), desc="åµŒå…¥å¹¶æ·»åŠ æ–°æ–‡æ¡£", unit="æ‰¹"):
        batch_docs = docs_to_add[i:i + batch_size]
        batch_ids = ids_to_add[i:i + batch_size]

        if batch_docs:
            try:
                # ä½¿ç”¨ add_documents æ—¶åŒæ—¶ä¼ å…¥ documents å’Œ ids
                db.add_documents(documents=batch_docs, ids=batch_ids)
            except Exception as e:
                print(f"âŒ åœ¨å¤„ç†æ‰¹æ¬¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                print("ç¨‹åºå°†åœæ­¢ã€‚å·²å®Œæˆçš„æ‰¹æ¬¡å·²ä¿å­˜ï¼Œä¸‹æ¬¡é‡æ–°è¿è¡Œæ­¤è„šæœ¬å³å¯ä»æ–­ç‚¹ç»§ç»­ã€‚")
                return

    print(f"âœ… æˆåŠŸæ·»åŠ  {len(docs_to_add)} ä¸ªæ–°æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼")

###æ–°å¢å†…å®¹è¿›çŸ¥è¯†åº“
def add_to_knowledge_base(excel_file: str, save_path: str):
    # åŠ è½½ç°æœ‰çŸ¥è¯†åº“
    embeddings = DashScopeEmbeddings()
    db = Chroma(persist_directory=save_path, embedding_function=embeddings)

    # è¯»å–æ–°çš„Excelæ–‡ä»¶
    df = pd.read_excel(excel_file)
    if "req_code" not in df.columns:
        raise ValueError("Excelä¸­å¿…é¡»åŒ…å« req_code åˆ—ï¼")

    new_docs = []
    for _, row in df.iterrows():
        req_code = str(row["req_code"]).strip()
        # æ‹¼æ¥å…¶ä»–åˆ—
        content_parts = []
        for col in df.columns:
            if col != "req_code":
                content_parts.append(f"{col}: {row[col]}")
        content = "\n".join(content_parts)
        new_docs.append(Document(page_content=req_code, metadata={"content": content}))

    # å°†æ–°æ–‡æ¡£æ·»åŠ åˆ°ç°æœ‰çŸ¥è¯†åº“
    db.add_documents(new_docs)
    print(f"âœ… æ–°å†…å®¹å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“ï¼")


# ============ æ£€ç´¢çŸ¥è¯†åº“ ============
def load_knowledge_base(save_path: str):
    embeddings = DashScopeEmbeddings()
    return Chroma(persist_directory=save_path, embedding_function=embeddings)

def search_knowledge_base(query: str, save_path: str, top_k: int = 3):
    db = load_knowledge_base(save_path)
    results = db.similarity_search(query, k=top_k)
    for i, res in enumerate(results, 1):
        print(f"ğŸ” ç»“æœ {i}:")
        print(f"req_code: {res.page_content}")
        print(f"å†…å®¹: {res.metadata['content']}\n")

# ============ ç¤ºä¾‹è¿è¡Œ ============
if __name__ == "__main__":
    excel_file = "./no_code_abstract_instance/Seam2.xlsx"  # è¾“å…¥çš„Excelæ–‡ä»¶
    save_path = "./knowledge_base/Seam2_no_ca"                   # çŸ¥è¯†åº“å­˜å‚¨è·¯å¾„

    # add_to_knowledge_base(excel_file, save_path)
    # ç¬¬ä¸€æ¬¡è¿è¡Œï¼šæ„å»ºçŸ¥è¯†åº“
    build_knowledge_base(excel_file, save_path)

    # æ£€ç´¢ç¤ºä¾‹
    # query = ""
    # search_knowledge_base(query, save_path, top_k=3)
